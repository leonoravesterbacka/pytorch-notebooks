{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch notebook for reweighting using density ratio estimation with calibrated classifiers\n",
    "\n",
    "The idea behind this notebook is to reweight one distribution $p_0(x)$ to look like another distribution $p_1(x)$.  \n",
    "\n",
    "The reweighting technique is based on [Approximating Likelihood Ratios with Calibrated Discriminative Classifiers](http://inspirehep.net/record/1377273). \n",
    "\n",
    "In this notebook V+jets samples generated with Madgraph5 ($p_0(x)$) and Sherpa ($p_1(x)$) are compared, and the  weights are derived to reweight Madgraph5 to look like Sherpa.  \n",
    "\n",
    "The performance of the weights, i.e. how well the reweighted original distribution matches the target distribution, is assessed by training a discriminator to differentiate the original distribution with weights applied from a target distribution.  \n",
    "\n",
    "Work in progress by Leonora Vesterbacka. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.18/00\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import theano\n",
    "from itertools import product\n",
    "import root_numpy\n",
    "import pandas as pd\n",
    "import uproot\n",
    "import torch\n",
    "np.random.seed(314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the example has much more real data than monte carlo\n",
    "#this leads to unbalanced dataset\n",
    "#some techniques deal with that better than others\n",
    "data_to_use = [\"all\",\"max balanced\"][0]\n",
    "\n",
    "# the histogram and kde calibration don't work very well\n",
    "#with very peaked output score distributions,\n",
    "#but the isotonic approach does\n",
    "calibration_type = [\"isotonic\", \"kde\", \"histogram\"][0]\n",
    "\n",
    "#do either training using all phase space by defining do = \"varAll\", or just two variables by defining do = \"var2\"\n",
    "do = [\"two\", \"all\"][0]\n",
    "normalize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('oT', tensor([[51.6451,  1.0000],\n",
      "        [ 2.4802,  0.0000],\n",
      "        [14.1178,  0.0000],\n",
      "        ...,\n",
      "        [71.5885,  0.0000],\n",
      "        [ 9.1785,  0.0000],\n",
      "        [10.2159,  0.0000]], dtype=torch.float64))\n"
     ]
    }
   ],
   "source": [
    "if do == \"two\":\n",
    "    binning = [range(0, 2400, 200), range(0, 15, 1)]\n",
    "    variables = ['VpT','Njets']\n",
    "    vlabels = ['V $\\mathrm{p_{T}}$ [GeV]','Number of jets']\n",
    "\n",
    "    weights = ['normweight']\n",
    "if do == \"all\":\n",
    "    etaV = [-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "    etaJ = [-2.8,-2.4,-2,-1.6,-1.2,-0.8,-0.4,0,0.4,0.8,1.2,1.6,2,2.4,2.8]\n",
    "    variables = ['VpT','Njets','j1pT', 'j2pT', 'HT','ptmiss', 'l1pT','Veta','j1eta','j2eta']\n",
    "    vlabels = ['V $\\mathrm{p_{T}}$ [GeV]','Number of jets','Leading jet $\\mathrm{p_{T}}$ [GeV]','Subleading jet $\\mathrm{p_{T}}$ [GeV]', '$\\mathrm{H_{T}}$ [GeV]','$\\mathrm{p_{T}^{miss}}$ [GeV]', 'Leading lepton $\\mathrm{p_{T}}$ [GeV]','V $\\eta$','Leading jet $\\eta$','Subleading jet $\\eta$']\n",
    "    binning = [range(0, 2400, 100), range(0, 15, 1), range(0, 2700, 100),range(0, 2700, 100),range(0, 4000, 200),range(0, 600, 50),range(0, 1500, 50), etaV, etaJ, etaJ]\n",
    "    weights = ['normweight']\n",
    "    #truthWeight is the generator weight\n",
    "#get original and target samples, madgraph:original, sherpa:target\n",
    "original  = root_numpy.root2array('/eos/user/m/mvesterb/data/madgraph/one/Nominal.root', branches=variables)\n",
    "target    = root_numpy.root2array('/eos/user/m/mvesterb/data/sherpa/one/Nominal.root', branches=variables)\n",
    "#originalW = root_numpy.root2array('/eos/user/m/mvesterb/data/madgraph/one/Nominal.root', branches=weights)\n",
    "#targetW   = root_numpy.root2array('/eos/user/m/mvesterb/data/sherpa/one/Nominal.root', branches=weights)\n",
    "#create dataframes to do the training on, and also get the sample weights in separate dataframes for resampling\n",
    "oDF   = pd.DataFrame(original,columns=variables)\n",
    "tDF   = pd.DataFrame(target,columns=variables)\n",
    "#oWDF  = pd.DataFrame(originalW,columns=weights)\n",
    "#tWDF  = pd.DataFrame(targetW,columns=weights)\n",
    "\n",
    "oT = torch.tensor(oDF[variables].values)\n",
    "tT = torch.tensor(tDF[variables].values)\n",
    "print(\"oT\",oT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A discriminator is trained to differentiate the original and the target distributions from each other, as well as differentiating the target distribution from the original distributions with the learned carl applied. Well learned weights would make the target and reweighted distributions very similar and indistinguishable for the discriminator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20981, 2])\n",
      "torch.Size([20981, 2])\n",
      "torch.Size([20981, 2])\n",
      "torch.Size([83316, 2])\n"
     ]
    }
   ],
   "source": [
    "#to randomize training and test data\n",
    "n_target = oT.shape[0]\n",
    "rand = np.random.choice(range(oT.shape[0]),2*n_target,replace=True)\n",
    "randomized_original = oT[rand]\n",
    "\n",
    "X0_all = randomized_original[:n_target,:]\n",
    "X0_test = randomized_original[n_target:,:]\n",
    "\n",
    "X1_all = tT\n",
    "print(oT.shape)\n",
    "print(X0_all.shape)\n",
    "print(X0_test.shape)\n",
    "print(X1_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20981, 2])\n",
      "torch.Size([83316, 2])\n",
      "torch.Size([104297])\n",
      "torch.Size([104297, 2])\n"
     ]
    }
   ],
   "source": [
    "#make training data from all samples\n",
    "num1 = X0_all.shape[0]\n",
    "num2 = X1_all.shape[0]\n",
    "\n",
    "#X_all = np.vstack((X0_all,X1_all))\n",
    "#y_all = np.ones(num1 + num2, dtype=np.int)\n",
    "X_all = torch.cat([X0_all,X1_all])\n",
    "y_all = torch.ones(num1+num2,dtype=torch.int) \n",
    "y_all[num1:] = 0\n",
    "\n",
    "#randomly sample X0 to have the same number of entries as X1\n",
    "# assuming X0 is bigger here\n",
    "X0_s = X0_all[np.random.choice(range(X0_all.shape[0]),num1,replace=True)]\n",
    "X_s = torch.cat((X0_s, X1_all))\n",
    "y_s = torch.ones(num1 + num2, dtype=torch.int)\n",
    "y_s[num1:] = 0\n",
    "\n",
    "X1_x = X1_all[np.random.choice(range(X1_all.shape[0]),num1,replace=True)]\n",
    "X_x = torch.cat((X0_all, X1_x))\n",
    "y_x = torch.ones(num1 + num1, dtype=torch.int)\n",
    "y_x[num1:] = 0\n",
    "#now use the flags to decide which of the datasets to use\n",
    "X, X0, X1, y = None, None, None, None\n",
    "if data_to_use == \"all\":\n",
    "    X, X0, X1, y = X_all, X0_all, X1_all, y_all\n",
    "elif data_to_use == \"max balanced\":\n",
    "    X, X0, X1, y = X_s, X0_s, X1_all, y_s\n",
    "else:    \n",
    "    print(\"error\")\n",
    "X0.requires_grad_(True)\n",
    "X1.requires_grad_(True)\n",
    "X.requires_grad_(True)\n",
    "\n",
    "print(X0.shape)\n",
    "print(X1.shape)\n",
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
