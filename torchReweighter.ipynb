{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch notebook for reweighting using density ratio estimation with calibrated classifiers\n",
    "\n",
    "The idea behind this notebook is to reweight one distribution $p_0(x)$ to look like another distribution $p_1(x)$.  \n",
    "\n",
    "The reweighting technique is based on [Approximating Likelihood Ratios with Calibrated Discriminative Classifiers](http://inspirehep.net/record/1377273). \n",
    "\n",
    "In this notebook V+jets samples generated with Madgraph5 ($p_0(x)$) and Sherpa ($p_1(x)$) are compared, and the  weights are derived to reweight Madgraph5 to look like Sherpa.  \n",
    "\n",
    "The performance of the weights, i.e. how well the reweighted original distribution matches the target distribution, is assessed by training a discriminator to differentiate the original distribution with weights applied from a target distribution.  \n",
    "\n",
    "Work in progress by Leonora Vesterbacka. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.18/00\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import theano\n",
    "from itertools import product\n",
    "import root_numpy\n",
    "import pandas as pd\n",
    "import uproot\n",
    "import torch\n",
    "np.random.seed(314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the example has much more real data than monte carlo\n",
    "#this leads to unbalanced dataset\n",
    "#some techniques deal with that better than others\n",
    "data_to_use = [\"all\",\"max balanced\"][0]\n",
    "\n",
    "# the histogram and kde calibration don't work very well\n",
    "#with very peaked output score distributions,\n",
    "#but the isotonic approach does\n",
    "calibration_type = [\"isotonic\", \"kde\", \"histogram\"][0]\n",
    "\n",
    "#do either training using all phase space by defining do = \"varAll\", or just two variables by defining do = \"var2\"\n",
    "do = [\"two\", \"all\"][0]\n",
    "normalize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('oT', tensor([[51.6451,  1.0000],\n",
      "        [ 2.4802,  0.0000],\n",
      "        [14.1178,  0.0000],\n",
      "        ...,\n",
      "        [71.5885,  0.0000],\n",
      "        [ 9.1785,  0.0000],\n",
      "        [10.2159,  0.0000]]))\n"
     ]
    }
   ],
   "source": [
    "if do == \"two\":\n",
    "    binning = [range(0, 2400, 200), range(0, 15, 1)]\n",
    "    variables = ['VpT','Njets']\n",
    "    vlabels = ['V $\\mathrm{p_{T}}$ [GeV]','Number of jets']\n",
    "\n",
    "    weights = ['normweight']\n",
    "if do == \"all\":\n",
    "    etaV = [-10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "    etaJ = [-2.8,-2.4,-2,-1.6,-1.2,-0.8,-0.4,0,0.4,0.8,1.2,1.6,2,2.4,2.8]\n",
    "    variables = ['VpT','Njets','j1pT', 'j2pT', 'HT','ptmiss', 'l1pT','Veta','j1eta','j2eta']\n",
    "    vlabels = ['V $\\mathrm{p_{T}}$ [GeV]','Number of jets','Leading jet $\\mathrm{p_{T}}$ [GeV]','Subleading jet $\\mathrm{p_{T}}$ [GeV]', '$\\mathrm{H_{T}}$ [GeV]','$\\mathrm{p_{T}^{miss}}$ [GeV]', 'Leading lepton $\\mathrm{p_{T}}$ [GeV]','V $\\eta$','Leading jet $\\eta$','Subleading jet $\\eta$']\n",
    "    binning = [range(0, 2400, 100), range(0, 15, 1), range(0, 2700, 100),range(0, 2700, 100),range(0, 4000, 200),range(0, 600, 50),range(0, 1500, 50), etaV, etaJ, etaJ]\n",
    "    weights = ['normweight']\n",
    "    #truthWeight is the generator weight\n",
    "#get original and target samples, madgraph:original, sherpa:target\n",
    "original  = root_numpy.root2array('/eos/user/m/mvesterb/data/madgraph/one/Nominal.root', branches=variables)\n",
    "target    = root_numpy.root2array('/eos/user/m/mvesterb/data/sherpa/one/Nominal.root', branches=variables)\n",
    "#originalW = root_numpy.root2array('/eos/user/m/mvesterb/data/madgraph/one/Nominal.root', branches=weights)\n",
    "#targetW   = root_numpy.root2array('/eos/user/m/mvesterb/data/sherpa/one/Nominal.root', branches=weights)\n",
    "#create dataframes to do the training on, and also get the sample weights in separate dataframes for resampling\n",
    "oDF   = pd.DataFrame(original,columns=variables)\n",
    "tDF   = pd.DataFrame(target,columns=variables)\n",
    "#oWDF  = pd.DataFrame(originalW,columns=weights)\n",
    "#tWDF  = pd.DataFrame(targetW,columns=weights)\n",
    "\n",
    "oT = torch.tensor(oDF[variables].values,dtype=torch.float)\n",
    "tT = torch.tensor(tDF[variables].values,dtype=torch.float)\n",
    "print(\"oT\",oT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A discriminator is trained to differentiate the original and the target distributions from each other, as well as differentiating the target distribution from the original distributions with the learned carl applied. Well learned weights would make the target and reweighted distributions very similar and indistinguishable for the discriminator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20981, 2])\n",
      "torch.Size([83316, 2])\n",
      "torch.Size([83316, 2])\n",
      "torch.Size([83316, 2])\n"
     ]
    }
   ],
   "source": [
    "#to randomize training and test data\n",
    "n_target = tT.shape[0]\n",
    "rand = np.random.choice(range(oT.shape[0]),2*n_target,replace=True)\n",
    "randomized_original = oT[rand]\n",
    "\n",
    "X0_all = randomized_original[:n_target,:]\n",
    "X0_test = randomized_original[-n_target:,:]\n",
    "\n",
    "X1_all = tT\n",
    "print(oT.shape)\n",
    "print(X0_all.shape)\n",
    "print(X0_test.shape)\n",
    "print(X1_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X', tensor([[ 7.3920,  0.0000],\n",
      "        [ 6.8687,  1.0000],\n",
      "        [34.5983,  0.0000],\n",
      "        ...,\n",
      "        [37.7138,  1.0000],\n",
      "        [10.6486,  0.0000],\n",
      "        [32.3756,  1.0000]], requires_grad=True))\n",
      "torch.Size([83316, 2])\n",
      "torch.Size([83316, 2])\n",
      "torch.Size([166632])\n",
      "torch.Size([166632, 2])\n"
     ]
    }
   ],
   "source": [
    "#make training data from all samples\n",
    "num1 = X0_all.shape[0]\n",
    "num2 = X1_all.shape[0]\n",
    "\n",
    "#X_all = np.vstack((X0_all,X1_all))\n",
    "#y_all = np.ones(num1 + num2, dtype=np.int)\n",
    "X_all = torch.cat([X0_all,X1_all])\n",
    "y_all = torch.ones(num1+num2,dtype=torch.float) \n",
    "y_all[num1:] = 0\n",
    "\n",
    "#randomly sample X0 to have the same number of entries as X1\n",
    "# assuming X0 is bigger here\n",
    "X0_s = X0_all[np.random.choice(range(X0_all.shape[0]),num1,replace=True)]\n",
    "X_s = torch.cat((X0_s, X1_all))\n",
    "y_s = torch.ones(num1 + num2, dtype=torch.int)\n",
    "y_s[num1:] = 0\n",
    "\n",
    "X1_x = X1_all[np.random.choice(range(X1_all.shape[0]),num1,replace=True)]\n",
    "X_x = torch.cat((X0_all, X1_x))\n",
    "y_x = torch.ones(num1 + num1, dtype=torch.int)\n",
    "y_x[num1:] = 0\n",
    "#now use the flags to decide which of the datasets to use\n",
    "X, X0, X1, y = None, None, None, None\n",
    "if data_to_use == \"all\":\n",
    "    X, X0, X1, y = X_all, X0_all, X1_all, y_all\n",
    "elif data_to_use == \"max balanced\":\n",
    "    X, X0, X1, y = X_s, X0_s, X1_all, y_s\n",
    "else:    \n",
    "    print(\"error\")\n",
    "X0.requires_grad_(True)\n",
    "X1.requires_grad_(True)\n",
    "X.requires_grad_(True)\n",
    "print(\"X\",X)\n",
    "X = torch.FloatTensor(X)\n",
    "X0 = torch.FloatTensor(X0)\n",
    "X1 = torch.FloatTensor(X1)\n",
    "y = torch.FloatTensor(y)\n",
    "\n",
    "print(X0.shape)\n",
    "print(X1.shape)\n",
    "print(y.shape)\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "## Define the NN architecture\n",
    "class Feedforward(torch.nn.Module):\n",
    "        def __init__(self, input_size, hidden_size):\n",
    "            super(Feedforward, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size  = hidden_size\n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.fc2 = torch.nn.Linear(self.hidden_size, 1)\n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "        def forward(self, x):\n",
    "            hidden = self.fc1(x)\n",
    "            relu = self.relu(hidden)\n",
    "            output = self.fc2(relu)\n",
    "            output = self.sigmoid(output)\n",
    "            return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 215.5257\n",
      "Epoch [11/200], Loss: 204.2476\n",
      "Epoch [21/200], Loss: 203.8199\n",
      "Epoch [31/200], Loss: 204.2938\n",
      "Epoch [41/200], Loss: 203.3584\n",
      "Epoch [51/200], Loss: 202.9949\n",
      "Epoch [61/200], Loss: 202.6688\n",
      "Epoch [71/200], Loss: 202.4050\n",
      "Epoch [81/200], Loss: 202.0863\n",
      "Epoch [91/200], Loss: 201.8631\n",
      "Epoch [101/200], Loss: 201.9190\n",
      "Epoch [111/200], Loss: 201.6627\n",
      "Epoch [121/200], Loss: 201.6057\n",
      "Epoch [131/200], Loss: 201.6184\n",
      "Epoch [141/200], Loss: 201.4360\n",
      "Epoch [151/200], Loss: 201.3977\n",
      "Epoch [161/200], Loss: 201.4014\n",
      "Epoch [171/200], Loss: 201.3519\n",
      "Epoch [181/200], Loss: 201.2463\n",
      "Epoch [191/200], Loss: 201.2540\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.18802762862010208"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as utils_data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "inputs = Variable(X)\n",
    "targets = Variable(y)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc2(F.relu(self.fc1(x)))\n",
    "        return out\n",
    "\n",
    "input_size = inputs.size()[1]\n",
    "hidden_size = 100\n",
    "output_size = 1\n",
    "num_epoch = 200\n",
    "learning_rate = 1e-3\n",
    "model = MLP(input_size = input_size, hidden_size = hidden_size,\n",
    "            output_size = output_size)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay=1e-4)\n",
    "loss_fct = nn.MSELoss()\n",
    "training_samples = utils_data.TensorDataset(inputs, targets)\n",
    "data_loader_trn = utils_data.DataLoader(training_samples, batch_size=200, drop_last=False, shuffle=True)\n",
    "\n",
    "#train\n",
    "for epoch in range(num_epoch):\n",
    "        cum_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(data_loader_trn):\n",
    "\n",
    "            X, y = data.float(), target.float()\n",
    "\n",
    "            pred = model(X)\n",
    "            loss = loss_fct(pred, y.unsqueeze(1)) \n",
    "\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cum_loss += loss.item()\n",
    "            \n",
    "        if epoch % 10 == 0:\n",
    "            print ('Epoch [%d/%d], Loss: %.4f' \n",
    "                      %(epoch+1, num_epoch, cum_loss))\n",
    "\n",
    "\n",
    "final_prediction = model(inputs)\n",
    "final_pred_np = final_prediction.clone().detach().numpy()\n",
    "\n",
    "np.corrcoef(final_pred_np.squeeze(), targets)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Feedforward(2, 10)\n",
    "# specify loss function\n",
    "criterion = torch.nn.BCELoss()\n",
    "# specify optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('model', Feedforward(\n",
      "  (fc1): Linear(in_features=2, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "))\n",
      "('Test loss before training', 2.37048077583313)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print(\"model\",model)\n",
    "y_pred = model(X)\n",
    "before_train = criterion(y_pred.squeeze(), y)\n",
    "print('Test loss before training' , before_train.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 0.693524181843\n",
      "Epoch 1: train loss: 0.693477332592\n",
      "Epoch 2: train loss: 0.693429529667\n",
      "Epoch 3: train loss: 0.693384587765\n",
      "Epoch 4: train loss: 0.693343222141\n",
      "Epoch 5: train loss: 0.693293750286\n",
      "Epoch 6: train loss: 0.693261802197\n",
      "Epoch 7: train loss: 0.693234980106\n",
      "Epoch 8: train loss: 0.693212747574\n",
      "Epoch 9: train loss: 0.693188965321\n",
      "Epoch 10: train loss: 0.693153619766\n",
      "Epoch 11: train loss: 0.693126380444\n",
      "Epoch 12: train loss: 0.693103075027\n",
      "Epoch 13: train loss: 0.693069756031\n",
      "Epoch 14: train loss: 0.69303894043\n",
      "Epoch 15: train loss: 0.693009018898\n",
      "Epoch 16: train loss: 0.692978978157\n",
      "Epoch 17: train loss: 0.692942619324\n",
      "Epoch 18: train loss: 0.692906439304\n",
      "Epoch 19: train loss: 0.692872643471\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epoch = 20\n",
    "for epoch in range(epoch):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred = model(X)\n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred.squeeze(), y)\n",
    "   \n",
    "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Test loss after Training', 0.6928380131721497)\n",
      "y_pred tensor([[0.4552],\n",
      "        [0.4532],\n",
      "        [0.4404],\n",
      "        ...,\n",
      "        [0.4315],\n",
      "        [0.4534],\n",
      "        [0.4344]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(X)\n",
    "after_train = criterion(y_pred.squeeze(), y) \n",
    "print('Test loss after Training' , after_train.item())\n",
    "print(\"y_pred\"), y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
